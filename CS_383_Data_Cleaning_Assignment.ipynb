{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Imports and pip installations (if needed)\n"
      ],
      "metadata": {
        "id": "sCuXLJwsodho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Load the dataset"
      ],
      "metadata": {
        "id": "QKX0OrMtqAKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the given datasets\n",
        "\n",
        "\n",
        "# Print the data\n"
      ],
      "metadata": {
        "id": "GR38S5mRo0r5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Analyze the Dataset"
      ],
      "metadata": {
        "id": "93V45BfjqI6i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refer to this: https://archive.ics.uci.edu/dataset/336/chronic+kidney+disease\n",
        "\n",
        "Explain what the each data is in your own words. What are the features and labels? Are the features in the given datasets : categorical, numerical or both? Give 3 examples of categorical and numerical columns each (if they exist)"
      ],
      "metadata": {
        "id": "08JU4hVnqMLF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:"
      ],
      "metadata": {
        "id": "HRSbXNfT09Wu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: Data Preprocessing\n",
        "\n",
        "A fundamental skill in Machine Learning is mastering the art of data cleaning and preprocessing. In this assignment, you will learn and apply essential data cleaning techniques to transform a raw dataset into a clean, ready-to-use form which you can use for regression or classification tasks. By the end of this assignment, you'll have a fully clean dataset and a solid foundation in preparing data for various machine learning models."
      ],
      "metadata": {
        "id": "R6raapHMwfID"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3.1 : Drop Duplicate rows\n",
        "\n",
        "Let's start by checking if the given datasets have any duplicate rows (same Unique Id). Use pandas to identify and remove these duplicate rows from the given dataset"
      ],
      "metadata": {
        "id": "t2FYmj6zTJk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For the numerical dataset, check if there are duplicate rows in the dataset. If yes, print total number of duplicate rows\n",
        "\n",
        "\n",
        "# Drop these duplicate rows\n",
        "\n",
        "\n",
        "# Repeat the same for categorical dataset. Print the duplicate rows and drop them\n",
        "\n"
      ],
      "metadata": {
        "id": "XjkwmspsQeXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3.2: Combine two differents datasets\n",
        "\n",
        "A good skill to have is to know how to combine 2 different datasets.\n",
        "\n",
        "Are all the unique ids are present in both datasets? Why do you think so? If not, what do the rows that are missing from one of the datasets look like in the combined table?"
      ],
      "metadata": {
        "id": "HBFadPsfBMYQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:"
      ],
      "metadata": {
        "id": "1KFoNLYtVYYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge the two given numerical and categorical datasets based on their unique_ID.\n",
        "\n",
        "#Print the combined dataset\n"
      ],
      "metadata": {
        "id": "JYDcaPFQBtbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3.3: Rows with Missing values\n",
        "\n",
        "Removing missing values from a dataset is important for classification because it ensures the model is trained on complete and accurate data, leading to better performance and reliable predictions. Incomplete data can introduce bias and errors, negatively impacting the model's effectiveness."
      ],
      "metadata": {
        "id": "RgKzV1mFwksD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the percentage of rows that contain atleast one missing value\n",
        "\n",
        "# Print %\n",
        "\n",
        "# Drop these rows from the dataset\n",
        "\n",
        "# Print the Dataset"
      ],
      "metadata": {
        "id": "octo6qxgxstJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3.4: Sort the dataset according to the Labels"
      ],
      "metadata": {
        "id": "QHLmE7i75-Xu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the dataset according to the values in 'Target' column. Make sure reset the indices after sorting\n",
        "\n",
        "# Print the dataset"
      ],
      "metadata": {
        "id": "CzvGw6Xt-M-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3.5: Encoding Categorical data\n",
        "\n",
        "In this step, we identify and process the categorical columns in the sorted dataset. We map each unique value in these columns to separate \"dummy\" columns.\n",
        "\n",
        "For example, the column 'rbc' will be transformed into two columns 'rbc_normal' and 'rbc_abnormal'. If a row's value in 'rbc' is 'normal', the 'rbc_normal' column will be set to 1 and 'rbc_abnormal' will be set to 0.\n",
        "\n",
        "\n",
        "**Note: Find a correct pandas function to do this **"
      ],
      "metadata": {
        "id": "WZWpxx0pzoRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write code here\n",
        "categorical_columns = ['rbc', 'pc', 'pcc', 'ba', 'htn', 'dm', 'cad', 'appet', 'pe', 'ane', 'Target']\n",
        "\n",
        "# Print the dataset"
      ],
      "metadata": {
        "id": "v9R8ldVIwoc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the example we went through above, another solution is to have a single column for the binary variable. In the downstream modeling would this be equivalent? What effect would this have if the categorical variable could take more than 2 values? For example, let's say we have a categorical feature that is \"type of condiment\" that can take 5 separate values and we are trying to predict the rating of a particular sandwich."
      ],
      "metadata": {
        "id": "_fvO_TZ0Uvgc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3.6 : Remove Outliers from Numerical Columns\n",
        "\n",
        "Outliers can disproportionately influence the fit of a regression model, potentially leading to a model that does not generalize well therefore it is important that we remove outliers from the numerical columns of the dataset.\n",
        "\n",
        "For this dataset, we define an outlier to be 3 times the standard deviation from the mean. Drop these outliers from the dataset"
      ],
      "metadata": {
        "id": "kocVyS5yA-Fv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove outliers\n",
        "numerical_columns = ['age', 'bp', 'bgr', 'bu', 'sc', 'sod', 'pot', 'hemo', 'pcv', 'wbcc', 'rbcc']\n",
        "\n",
        "# Print the dataset"
      ],
      "metadata": {
        "id": "TdupS0h3h6Cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3.7 : Normalize the Numerical Columns\n",
        "\n",
        "Normalizing numerical attributes ensures that all features contribute equally to the model by scaling them to a consistent range, which improves model performance and convergence. It prevents features with larger scales from disproportionately influencing the model's learning process."
      ],
      "metadata": {
        "id": "8SSSwLfFjpL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the all Numerical Attributes in the dataset.\n",
        "\n",
        "# Print the dataset"
      ],
      "metadata": {
        "id": "R4xGDDewA859"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3.8: Remove Unnecessary columns"
      ],
      "metadata": {
        "id": "fomr0fL847aq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are there any columns in this dataset which are not appropriate for modeling and predictions? Which column(s)? Justify their exclusion and remove them"
      ],
      "metadata": {
        "id": "rapn8p607X1e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:"
      ],
      "metadata": {
        "id": "rJ1r6z118C9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove that column\n",
        "\n",
        "# Print the dataset"
      ],
      "metadata": {
        "id": "8QIC2yyk5QO4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}